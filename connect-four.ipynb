{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1046cbbe",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2021 Giovanni Squillero `<squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see 'LICENCE.md' for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b93b7f",
   "metadata": {},
   "source": [
    "# Connect 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffa1907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da977d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, rows=6, cols=7):\n",
    "        self._rows = rows\n",
    "        self._cols = cols\n",
    "        self._nb = 4\n",
    "        self._board = None\n",
    "        self._starter = None\n",
    "        self._turn = None\n",
    "        self._won = None\n",
    "        self.last_move = None\n",
    "        self.create_game()\n",
    "\n",
    "    def create_game(self):\n",
    "        \"\"\"Creates the game state (board and variables)\"\"\"\n",
    "        self._board = self._board = np.zeros((self._cols, self._rows), dtype=np.byte)\n",
    "        self._starter = 1\n",
    "        self._turn = self._starter\n",
    "        self._won = None\n",
    "\n",
    "    def valid_moves(self):\n",
    "        \"\"\"Returns columns where a disc may be played\"\"\"\n",
    "        return [n for n in range(self._cols) if self._board[n, self._rows - 1] == 0]\n",
    "\n",
    "    def play(self, column, player):\n",
    "        \"\"\"Updates `board` as `player` drops a disc in `column`\"\"\"\n",
    "        (index,) = next((i for i, v in np.ndenumerate(self._board[column]) if v == 0))\n",
    "        self._board[column, index] = player\n",
    "        self.last_move = [column, index]\n",
    "\n",
    "    def take_back(self, column):\n",
    "        \"\"\"Updates `board` removing top disc from `column`\"\"\"\n",
    "        (index,) = [i for i, v in np.ndenumerate(self._board[column]) if v != 0][-1]\n",
    "        self._board[column, index] = 0\n",
    "        self.last_move = None\n",
    "\n",
    "    def four_in_a_row(self, player):\n",
    "        \"\"\"Checks if `player` has a 4-piece line\"\"\"\n",
    "        return (\n",
    "            any(\n",
    "                all(self._board[c, r] == player)\n",
    "                for c in range(self._cols)\n",
    "                for r in (list(range(n, n + self._nb)) for n in range(self._rows - self._nb + 1))\n",
    "            )\n",
    "            or any(\n",
    "                all(self._board[c, r] == player)\n",
    "                for r in range(self._rows)\n",
    "                for c in (list(range(n, n + self._nb)) for n in range(self._cols - self._nb + 1))\n",
    "            )\n",
    "            or any(\n",
    "                np.all(self._board[diag] == player)\n",
    "                for diag in (\n",
    "                    (range(ro, ro + self._nb), range(co, co + self._nb))\n",
    "                    for ro in range(0, self._cols - self._nb + 1)\n",
    "                    for co in range(0, self._rows - self._nb + 1)\n",
    "                )\n",
    "            )\n",
    "            or any(\n",
    "                np.all(self._board[diag] == player)\n",
    "                for diag in (\n",
    "                    (range(ro, ro + self._nb), range(co + self._nb - 1, co - 1, -1))\n",
    "                    for ro in range(0, self._cols - self._nb + 1)\n",
    "                    for co in range(0, self._rows - self._nb + 1)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def move(self, move):\n",
    "        self.play(move, self._turn)\n",
    "        score = self.eval_board(self._turn)\n",
    "        if score == 1 or score == -1:\n",
    "            self._won = score\n",
    "        self._turn = - self._turn\n",
    "\n",
    "    def _mc(self, player):\n",
    "        board = self._board.copy()\n",
    "        p = -player\n",
    "        while self.valid_moves():\n",
    "            p = -p\n",
    "            c = np.random.choice(self.valid_moves())\n",
    "            self.play(c, p)\n",
    "            if self.four_in_a_row(p):\n",
    "                self._board = board\n",
    "                return p\n",
    "        self._board = board\n",
    "        return 0\n",
    "\n",
    "    def montecarlo(self, player):\n",
    "        montecarlo_samples = 100\n",
    "        cnt = Counter(self._mc(player) for _ in range(montecarlo_samples))\n",
    "        return (cnt[1] - cnt[-1]) / montecarlo_samples\n",
    "\n",
    "    def eval_board(self, player):\n",
    "        if self.four_in_a_row(1):\n",
    "            return 1\n",
    "        elif self.four_in_a_row(-1):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.montecarlo(player)\n",
    "\n",
    "    def copy_state(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def get_win(self):\n",
    "        return self._won\n",
    "\n",
    "    def get_turn(self):\n",
    "        return self._turn\n",
    "\n",
    "    def get_board(self):\n",
    "        return self._board.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7f30f",
   "metadata": {},
   "source": [
    "## Minimax\n",
    "\n",
    "Implementation of the MinMax algorithm with varying depth using alpha-beta pruning.\n",
    "\n",
    "The goal of the MiniMax algorithm is to indicate to a player which move to make in order to maximize its chances of winning the whole game. According to the description of this algorithm, at each turn of a player and starting from the current board game, the code will simulate alternatively the possible moves of the player along with the ones of its adversary with the goal of maximizing the chances of winning of the first while minimizing the ones of the latter. At each depth of the simulation, the algorithm will simulate up to 7 boards, each having a piece dropped in one of the free columns. Through the MonteCarlo Evaluation provided each board will be attributed a score. After creating all the boards of the tree, at each depth, a board will be selected based on its reward and on the type of node (min or max) starting from the bottom of the tree (minimise or maximise the rewards depending on the depth which can be odd or even corresponding to the player or its adversary). The final choice is made among the 7 accessible boards from the current one with the score updated through the reward procedure described above.\n",
    "\n",
    "To get the adversary to play as an optimal player, each of the two players will be in turn considered as the maximizing player, hence trying to maximise the reward when a max nodes is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a7f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimax:\n",
    "    def __init__(self, game):\n",
    "        self._game = game\n",
    "\n",
    "    def minimax(self, depth, alpha, beta, maximizingPlayer):\n",
    "        valid_locations = self._game.valid_moves()\n",
    "        val = self._game.eval_board(self._game.get_turn())\n",
    "\n",
    "        if depth == 0 or not valid_locations:\n",
    "            if not valid_locations:\n",
    "                if self._game.get_win() == self._game.get_turn():\n",
    "                    return (None, float(\"inf\"))\n",
    "                elif self._game.get_win() == -self._game.get_turn():\n",
    "                    return (None, float(\"-inf\"))\n",
    "                else:\n",
    "                    return (None, 0)\n",
    "            else:\n",
    "                return None, val\n",
    "\n",
    "        elif maximizingPlayer:\n",
    "            val = float(\"-inf\")\n",
    "            column = np.random.choice(valid_locations)\n",
    "            for l in valid_locations:\n",
    "                self._game.play(l, self._game.get_turn())\n",
    "                new_score = self.minimax(depth - 1, alpha, beta, False)[1]\n",
    "                self._game.take_back(l)\n",
    "                if new_score > val:\n",
    "                    val = new_score\n",
    "                    column = l\n",
    "                alpha = max(alpha, val)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "            return column, val\n",
    "\n",
    "        else:\n",
    "            val = float(\"inf\")\n",
    "            column = np.random.choice(valid_locations)\n",
    "            for l in valid_locations:\n",
    "                self._game.play(l, -self._game.get_turn())\n",
    "                new_score = self.minimax(depth - 1, alpha, beta, True)[1]\n",
    "                self._game.take_back(l)\n",
    "                if new_score < val:\n",
    "                    val = new_score\n",
    "                    column = l\n",
    "                beta = min(beta, val)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "            return column, val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209ae87",
   "metadata": {},
   "source": [
    "### AI vs AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbefccd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 won !\n",
      "[[-1 -1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "game = Game()\n",
    "depth = 2\n",
    "\n",
    "while not game.get_win():\n",
    "    solver = Minimax(game)\n",
    "    move, val = solver.minimax(depth, float(\"-inf\"), float(\"inf\"), True)\n",
    "    game.move(move)\n",
    "\n",
    "if game.get_win() == 1:\n",
    "    print(\"Player 1 won !\")\n",
    "else:\n",
    "    print(\"Player 1 lost...\")\n",
    "print(game.get_board())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb4cf46",
   "metadata": {},
   "source": [
    "### AI vs Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58ce5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "0\n",
      "[[-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "5\n",
      "[[-1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "2\n",
      "[[-1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "0\n",
      "[[-1 -1  1  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "5\n",
      "[[-1 -1  1  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1 -1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "4\n",
      "[[-1 -1  1  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [-1 -1  1  1  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "3\n",
      "[[-1 -1  1  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1 -1  1  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [-1 -1  1  1  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "6\n",
      "[[-1 -1  1  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1 -1  1  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [-1 -1  1  1  0  0]\n",
      " [-1  1  0  0  0  0]]\n",
      "1\n",
      "[[-1 -1  1  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1 -1  1  1  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [-1 -1  1  1  0  0]\n",
      " [-1  1  0  0  0  0]]\n",
      "1\n",
      "[[-1 -1  1  0  0  0]\n",
      " [ 1 -1 -1  0  0  0]\n",
      " [-1  1  0  0  0  0]\n",
      " [ 1 -1  1  1  0  0]\n",
      " [ 1 -1  1  0  0  0]\n",
      " [-1 -1  1  1  0  0]\n",
      " [-1  1  0  0  0  0]]\n",
      "2\n",
      "[[-1 -1  1  0  0  0]\n",
      " [ 1 -1 -1  0  0  0]\n",
      " [-1  1 -1  0  0  0]\n",
      " [ 1 -1  1  1  0  0]\n",
      " [ 1 -1  1  1  0  0]\n",
      " [-1 -1  1  1  0  0]\n",
      " [-1  1  0  0  0  0]]\n",
      "6\n",
      "Player 1 won !\n",
      "[[-1 -1  1  0  0  0]\n",
      " [ 1 -1 -1  0  0  0]\n",
      " [-1  1 -1  0  0  0]\n",
      " [ 1 -1  1  1  0  0]\n",
      " [ 1 -1  1  1  0  0]\n",
      " [-1 -1  1  1  0  0]\n",
      " [-1  1 -1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "game = Game()\n",
    "depth = 2\n",
    "\n",
    "while not game.get_win():\n",
    "    solver = Minimax(game)\n",
    "    move, val = solver.minimax(depth, float(\"-inf\"), float(\"inf\"), True)\n",
    "    game.move(move)\n",
    "    print(game.get_board())\n",
    "    move = int(input())\n",
    "    game.move(move)\n",
    "\n",
    "if game.get_win() == 1:\n",
    "    print(\"Player 1 won !\")\n",
    "else:\n",
    "    print(\"Player 1 lost...\")\n",
    "print(game.get_board()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f6b1a",
   "metadata": {},
   "source": [
    "## MonteCarlo Tree Search\n",
    "\n",
    "### Node\n",
    "\n",
    "To implement the Monte-Carlo Tree Search algorithm, we first need to create the nodes of the tree of boards used during Monte-Carlo Tree Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256283db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.visits = 1\n",
    "        self.reward = 0.0\n",
    "        self.state = state\n",
    "        self.children = []\n",
    "        self.children_moves = []\n",
    "        self.parent = parent\n",
    "\n",
    "    def add_child(self, child_state, move):\n",
    "        \"\"\" Add a child to the current node \"\"\"\n",
    "        child = Node(child_state, parent=self)\n",
    "        self.children.append(child)\n",
    "        self.children_moves.append(move)\n",
    "\n",
    "    def update(self, reward):\n",
    "        \"\"\" Update the node's reward \"\"\"\n",
    "        self.reward += reward\n",
    "        self.visits += 1\n",
    "\n",
    "    def fully_explored(self):\n",
    "        \"\"\" Check if the node is fully explored (i.e. we cannot add any other children to this node) \"\"\"\n",
    "        if len(self.children) == len(self.state.valid_moves()):\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e840efd",
   "metadata": {},
   "source": [
    "### MCTS\n",
    "\n",
    "Implementation of the Monte Carlo Tree Search algorithm.\n",
    "\n",
    "This algorithm performs series of selection, expansion, random simulation and backpropagation for a given number of\n",
    "iterations (the more iterations, the better the result, but the slower the execution).\n",
    "A reward system is implemented allowing to decide the best move to make next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f7619d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo:\n",
    "    def __init__(self, game):\n",
    "        self._game = game\n",
    "\n",
    "    def monte_carlo_tree_search(self, iterations, root, exploration_parameter):\n",
    "        \"\"\" Monte-Carlo Tree Search algorithm \"\"\"\n",
    "        for i in range(iterations):\n",
    "            node, turn = self.selection(root, 1, exploration_parameter)\n",
    "            reward = self.simulation(node.state, turn)\n",
    "            self.backpropagation(node, reward, turn)\n",
    "\n",
    "        ans = self.best_child(root, 0)\n",
    "        return ans.state.last_move[0]\n",
    "\n",
    "    def selection(self, node, turn, exploration_parameter):\n",
    "        \"\"\" Expand a node and take the best child until a winning state is reached \"\"\"\n",
    "        while not node.state.last_move or not node.state.four_in_a_row(turn):\n",
    "            if not node.fully_explored():\n",
    "                return self.expansion(node), -turn\n",
    "            else:\n",
    "                node = self.best_child(node, exploration_parameter)\n",
    "                turn *= -1\n",
    "\n",
    "        return node, turn\n",
    "\n",
    "    def expansion(self, node):\n",
    "        \"\"\" Add a child state to the node \"\"\"\n",
    "        valid_locations = node.state.valid_moves()\n",
    "        for col in valid_locations:\n",
    "            if col not in node.children_moves:\n",
    "                new_state = node.state.copy_state()\n",
    "                new_state.move(col)\n",
    "                break\n",
    "\n",
    "        node.add_child(new_state, col)\n",
    "        return node.children[-1]\n",
    "\n",
    "    def simulation(self, state_init, turn):\n",
    "        \"\"\" Simulates random moves until the game is won by someone and returns a reward \"\"\"\n",
    "        state = state_init.copy_state()\n",
    "        while not state.last_move or not state.four_in_a_row(turn):\n",
    "            free_cols = state.valid_moves()\n",
    "            col = np.random.choice(free_cols)\n",
    "            state.move(col)\n",
    "            turn *= -1\n",
    "\n",
    "        reward_bool = state.four_in_a_row(turn)\n",
    "        if reward_bool and turn == -1:\n",
    "            reward = 1\n",
    "        elif reward_bool and turn == 1:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "\n",
    "    def backpropagation(self, node, reward, turn):\n",
    "        \"\"\" Update the rewards of all the ancestors of a node \"\"\"\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            node.reward -= turn*reward\n",
    "            node = node.parent\n",
    "            turn *= -1\n",
    "        return\n",
    "\n",
    "    def best_child(self, node, exploration_parameter):\n",
    "        \"\"\" Returns the best child of a node based on a scoring system proposed by Auer, Cesa-Bianchi and Fischer \"\"\"\n",
    "        best_score = float('-inf')\n",
    "        best_children = []\n",
    "        for c in node.children:\n",
    "            exploitation = c.reward / c.visits\n",
    "            exploration = math.sqrt(math.log(2.0*node.visits)/float(c.visits))\n",
    "            score = exploitation + exploration_parameter*exploration\n",
    "            if score == best_score:\n",
    "                best_children.append(c)\n",
    "            if score > best_score:\n",
    "                best_children = [c]\n",
    "                best_score = score\n",
    "        res = np.random.choice(best_children)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5615912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1 won !\n",
      "[[ 1  0  0  0  0  0]\n",
      " [ 1 -1  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "game = Game()\n",
    "iteration = 2\n",
    "\n",
    "while not game.get_win():\n",
    "    o = Node(game.copy_state())\n",
    "    solver = MonteCarlo(game) \n",
    "    move = solver.monte_carlo_tree_search(iteration, o, 2.0)\n",
    "    game.move(move)\n",
    "    \n",
    "if game.get_win() == 1:\n",
    "    print(\"Player 1 won !\")\n",
    "else:\n",
    "    print(\"Player 1 lost...\")\n",
    "print(game.get_board())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304eaad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
